{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Продвинутый анализ данных для юристов\n",
    "\n",
    "## Обработка текстов\n",
    "\n",
    "### Токенизация, лемматизация, стемминг, стоп-слова. Векторизация (TF-IDF), Word2Vec\n",
    "\n",
    "#### Матвей Бакшук, преподаватель ФКН НИУ ВШЭ\n",
    "\n",
    "\n",
    "Материал подготовлен с использованием [статьи на хабр](https://habr.com/ru/companies/otus/articles/755772/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = 'justify'> Изучение текстовых данных является одной из фундаментальных задач в области анализа данных и машинного обучения. Однако тексты представляют собой сложные и многомерные структуры, которые не могут быть напрямую обработаны алгоритмами машинного обучения. В этом контексте извлечение признаков — это процесс преобразования текстовых данных в числовые векторы, которые могут быть использованы для обучения моделей и анализа. Этот шаг играет ключевую роль в предварительной обработке данных перед применением алгоритмов. </p>\n",
    "\n",
    "<p align = 'justify'><code>Term Frequency-Inverse Document Frequency (TF-IDF)</code> — это один из наиболее распространенных и мощных методов для извлечения признаков из текстовых данных. <code>TF-IDF</code> вычисляет важность каждого слова в документе относительно количества его употреблений в данном документе и во всей коллекции текстов. Этот метод позволяет выделить ключевые слова и понять, какие слова имеют больший вес для определенного документа в контексте всей коллекции. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала давайте разберёмся с используемыми понятиями и посмотрим на простую формулу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Термины `TF (Term Frequency)` и `IDF (Inverse Document Frequency)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>TF (Частота термина)</b> обозначает, насколько часто определенное слово появляется в данном документе. Таким образом, TF измеряет важность слова в контексте отдельного документа.</li>\n",
    "\n",
    "<li><b> IDF (Обратная частота документа)</b> измеряет, насколько уникально слово является по всей коллекции документов. Слова, которые появляются в большинстве документов, имеют низкое IDF, так как они не вносят большой информационной ценности.</li>\n",
    "</ul>\n",
    "\n",
    "Формула `TF-IDF` комбинирует понятия `TF` и `IDF`, чтобы вычислить важность каждого слова в каждом документе. Формально, формула выглядит следующим образом:\n",
    "\n",
    "$$TF\\text{-}IDF(t, d) = TF(t, d) * IDF(t)$$\n",
    "\n",
    "где:\n",
    "\n",
    "$TF(t, d)$ - Частота термина $(TF)$ для слова $t$ в документе $d$\n",
    "\n",
    "$IDF(t)$ - Обратная частота документа $(IDF)$ для слова $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = 'justify'>Давайте рассмотрим простой корпус текстов и на его примере посмотрим на применение методов <code>TF-IDF</code>. Возьмем такие маленькие тексты:</p>\n",
    "\n",
    "<ol>\n",
    "<li>'Машинное обучение — это интересная область.'</li>\n",
    "\n",
    "<li>'Обучение с учителем — ключевой аспект машинного обучения.'</li>\n",
    "\n",
    "<li>'Область NLP также связана с машинным обучением.'</li>\n",
    "</ol>\n",
    "\n",
    "<p align = 'justify'>Перед тем как вычислять <code>TF-IDF</code>, мы должны выполнить предварительную обработку, такую как удаление стоп-слов, приведение к нижнему регистру и <b>токенизация</b> — разбиение текстов на отдельные слова или токены.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Расчет TF-IDF для слов в корпусе\n",
    "Представим, что мы хотим вычислить `TF-IDF` для слова \"машинное\" в первом документе. Давайте предположим, что `TF` для этого слова равен 1 (поскольку оно встречается 1 раз в данном документе), а `IDF` можно вычислить как общее количество документов (3) деленное на количество документов, в которых встречается это слово (2). Таким образом, IDF для слова \"машинное\" равен $log(\\frac{3}{2}) = 0.18$ (про появление здесь логарифма поговорим в следующем блоке)\n",
    "\n",
    "Теперь мы можем вычислить `TF-IDF` для слова \"машинное\" в первом документе: $$TF-IDF = 1 * 0.18 = 0.18$$\n",
    "Продолжая этот процесс для каждого слова в каждом документе, мы можем создать матрицу `TF-IDF`, где строки представляют слова, а столбцы - документы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логарифм в формуле IDF (Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Сглаживание весов**:\n",
    "   - Без логарифма `IDF` вычислялся бы как $ \\dfrac{N}{df_t} $, где $ N $ — общее число документов, а $ df_t $ — число документов, содержащих слово $ t $.\n",
    "   - Это значение может быть очень большим для редких слов (например, если слово встречается в 1 документе из 1000, `IDF` = 1000). Логарифм смягчает эту разницу, делая веса более управляемыми.\n",
    "\n",
    "2. **Учет \"информативности\"**:\n",
    "   - Логарифм отражает идею, что разница между 1 и 10 документами (где слово встречается) гораздо значимее, чем между 100 и 110. Это соответствует принципу уменьшающейся полезности (diminishing returns).\n",
    "\n",
    "3. **Математическая устойчивость**:\n",
    "   - Логарифм предотвращает доминирование редких слов с очень высоким IDF над частыми словами. Без него редкие термины могли бы искусственно завышать вес в итоговом TF-IDF.\n",
    "\n",
    "### Формула IDF:\n",
    "Обычно используется один из вариантов:\n",
    "\n",
    "$$\\text{IDF} = \\log\\left(\\frac{N}{df_t}\\right) \\quad \\text{или} \\quad \\log\\left(\\frac{N}{df_t} + 1\\right)$$\n",
    "где $ N $ — общее число документов, $ df_t $ — число документов со словом $ t $ .\n",
    "\n",
    "Логарифмическая шкала лучше соответствует человеческому восприятию \"важности\". Например:\n",
    "- Если слово есть во всех документах ($df_t = N$), `IDF` становится $\\log(1) = 0$ — такое слово не несет полезной информации для различения документов.\n",
    "- Редкие слова получают умеренно высокий вес, а не экстремальный.\n",
    "\n",
    "Это делает `TF-IDF` более устойчивым и интерпретируемым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Преимущества и ограничения TF-IDF\n",
    "\n",
    "Преимущества использования `TF-IDF` для извлечения признаков\n",
    "`TF-IDF` предоставляет несколько ключевых преимуществ:\n",
    "\n",
    "1. **Учет важности слов:** `TF-IDF` учитывает как частоту слова в документе, так и его общую редкость по всей коллекции. Таким образом, он помогает выделять ключевые слова, которые часто встречаются в данном документе, но не слишком распространены в остальных.\n",
    "\n",
    "2. **Устранение шума:** Слова, которые встречаются в большинстве документов (стоп-слова), имеют низкий `IDF` и, следовательно, низкий общий вес `TF-IDF`. Это позволяет устранить шум и фокусироваться на более важных словах.\n",
    "\n",
    "### Ограничения метода и ситуации, в которых он может быть неэффективен\n",
    "\n",
    "1. **Отсутствие семантической информации:** `TF-IDF` не учитывает семантические связи между словами, что может привести к ограниченной способности понимания смысла текста.\n",
    "\n",
    "2. **Чувствительность к длине документа:** Длинные документы могут иметь более высокие значения `TF`, даже если ключевые слова встречаются реже. В таких случаях, `TF-IDF` может недооценить важность конкретных слов.\n",
    "\n",
    "Важно понимать, в каких ситуациях `TF-IDF` будет эффективен, а когда стоит рассмотреть альтернативные методы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Еще один простой пример:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](https://github.com/M4tthew27/DATA/blob/main/tfidf.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применение TF-IDF в задачах анализа текстов\n",
    "\n",
    "#### A. Извлечение ключевых слов и терминов\n",
    "\n",
    "Извлечение ключевых слов из текстовых данных является одним из наиболее распространенных сценариев использования `TF-IDF`. Одним из способов сделать это - это выбрать топ-N слов с наибольшими значениями `TF-IDF`. Давайте рассмотрим пример с кодом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sklearn или !pip install sklearn\n",
    "# pip install --upgrade numpy scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Пример текстовых данных\n",
    "documents = [\n",
    "    \"Машинное обучение - это интересная область.\",\n",
    "    \"Обучение с учителем - ключевой аспект машинного обучения.\",\n",
    "    \"Область NLP также связана с машинным обучением.\"\n",
    "]\n",
    "\n",
    "# Создание объекта TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Применение TF-IDF к текстовым данным\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(f'Формат изначальной матрицы:\\n{tfidf_matrix}') # формат \"разреженной матрицы: большинство значений нули.\"\n",
    "# i - документ, j - термин (слово)\n",
    "\n",
    "# Получаем список терминов (слов)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для примера, давайте посмотрим на получение списка ключевых слов и их значения TF-IDF для первого документа\n",
    "\n",
    "tfidf_scores = tfidf_matrix.toarray()[0] # tfidf_matrix.toarray() позволяет отразить нулевые значения, [0] — первый документ\n",
    "\n",
    "# Посмотрим на два объекта, которые у нас получились:\n",
    "print(f'Результаты: {tfidf_scores}')\n",
    "print(f'Слова: {feature_names}')\n",
    "\n",
    "# Сортировка слов по значениям TF-IDF\n",
    "sorted_keywords = [word for i, word in sorted(zip(tfidf_scores, feature_names), reverse=True)]\n",
    "\n",
    "print(\"Ключевые слова из корпуса текстов в отсортированном порядке\", sorted_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применение ключевых слов в поисковых системах\n",
    "Извлечение ключевых слов с помощью `TF-IDF` позволяет улучшить поиск и индексацию текстовых данных. Представьте, что вы хотите построить поисковую систему для коллекции документов. Вы можете извлечь ключевые слова для каждого документа с помощью `TF-IDF` и использовать их для индексации и ранжирования результатов поиска.\n",
    "\n",
    "\n",
    "### B. Кластеризация и категоризация текстовых данных\n",
    "\n",
    "#### Группировка текстов по сходству на основе `TF-IDF`\n",
    "\n",
    "`TF-IDF` также может быть использован для кластеризации текстовых данных, то есть группировки схожих документов в один кластер. Кластеризация может помочь выявить общие темы и понять структуру данных. Рассмотрим следующий пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Применение кластеризации KMeans к матрице TF-IDF\n",
    "\n",
    "num_clusters = 2 # установим 2, поскольку у нас всего три текста\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0) \n",
    "\n",
    "# Кратко про метод k-средних: предоставляет разбиение набора данных на k кластеров, \n",
    "# таких, что каждый объект принадлежит кластеру с ближайшим центром кластера. Иными словами, мы ищем \n",
    "# скопление близких наблюдений в данных, чтобы добиться деления на группы (в данном примере у нас это)\n",
    "# группы текстов.\n",
    "\n",
    "# Цель алгоритма — минимизировать суммарное расстояние точек кластеров от их центров (частно: выделить схожие между\n",
    "# собой документы.\n",
    "\n",
    "## Заполняем на основании нашей матрицы:\n",
    "\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Чтобы показать примеры документов в каждом кластере\n",
    "\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "    print(f\"Кластер {cluster_id + 1}:\")\n",
    "    for idx in cluster_indices:\n",
    "        print(documents[idx])\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На пример трёх документов это просто и может казаться очевидным, однако на реальной практике данных больше, и число кластеров также может быть настроено"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практические примеры применения кластеризации\n",
    "\n",
    "+ **Новостные порталы**: новостные статьи могут быть автоматически категоризированы по темам с использованием кластеризации на основе TF-IDF.\n",
    "\n",
    "+ **Социальные сети**: кластеризация сообщений или постов пользователей может помочь создать персонализированные ленты новостей.\n",
    "\n",
    "##### Юриспруденция: \n",
    "\n",
    "+ **Автоматическая категоризация правовых документов**: кластеризация судебных решений, договоров или нормативных актов по темам (например: \"налоговые споры\", \"трудовые конфликты\", \"арбитражные дела\").\n",
    "\n",
    "+ **Анализ судебной практики**: группировка схожих судебных решений по ключевым параметрам (статьям закона, сумме иска, категории ответчика). Например, выявление кластеров дел, по которым чаще всего выносятся оправдательные приговоры. \n",
    "\n",
    "И т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Классификация текстов\n",
    "\n",
    "##### Обучение модели классификации на основе `TF-IDF`\n",
    "\n",
    "`TF-IDF` также может быть использован для классификации текстов на основе их содержания. Для этого мы можем обучить модель машинного обучения на векторах `TF-IDF`, представляющих документы, и затем использовать эту модель для предсказания категории новых текстов. Пример:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Пример данных для классификации\n",
    "categories = [\"технологии\", \"спорт\"]\n",
    "data = [\n",
    "    # Технологии\n",
    "    \"Новая модель смартфона вышла на рынок с улучшенной камерой.\",\n",
    "    \"Разработка искусственного интеллекта активно продвигается.\",\n",
    "    \"Компания представила инновационный чип для ноутбуков.\",\n",
    "    \"Учёные создали робота, способного выполнять хирургические операции.\",\n",
    "    \"Вышло обновление операционной системы с новыми функциями.\",\n",
    "    \"Разработан квантовый процессор нового поколения.\",\n",
    "    \"Инженеры тестируют дроны для доставки товаров.\",\n",
    "    \"Крупная ИТ-компания инвестирует в развитие облачных сервисов.\",\n",
    "    \"Появился новый язык программирования для веб-разработки.\",\n",
    "    \"Ведется разработка гибких дисплеев для смартфонов.\",\n",
    "    # Спорт\n",
    "    \"Сборная России победила в международном турнире по футболу.\",\n",
    "    \"Олимпийские игры пройдут в следующем году в Париже.\",\n",
    "    \"Теннисист выиграл престижный турнир в Лондоне.\",\n",
    "    \"Формула-1 представила обновлённый регламент гонок.\",\n",
    "    \"Хоккейная команда одержала уверенную победу в плей-офф.\",\n",
    "    \"Боксёр нокаутировал соперника в первом раунде.\",\n",
    "    \"Соревнования по легкой атлетике прошли в Москве.\",\n",
    "    \"Футболист перешёл в новый клуб за рекордную сумму.\",\n",
    "    \"Прошел финал чемпионата мира по баскетболу.\",\n",
    "    \"Пловец установил новый мировой рекорд.\"\n",
    "]\n",
    "\n",
    "labels = [0]*10 + [1]*10 # ставим 0 для технологий и 1 для спорта\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=0, stratify=labels) \n",
    "# делим \n",
    "\n",
    "\n",
    "# Преобразование текстовых данных в матрицу TF-IDF\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) # обучаем векторизатор\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test) # просто трансформируем в матрицу, тк предсказывать будем по обученной модели\n",
    "\n",
    "# Обучение модели классификации\n",
    "classifier = MultinomialNB() # берем просто для примера, тк позволяет работать с дискретными признаками\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Предсказание категорий для тестовых данных\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Оценка точности модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Точность модели: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сопоставление TF-IDF с Word2Vec, Doc2Vec и другими методами\n",
    "\n",
    "Существует множество других методов извлечения признаков из текста, таких как `Word2Vec`, `Doc2Vec`, `FastText` и многие другие. Эти методы, в отличие от `TF-IDF`, учитывают семантические связи между словами и векторное представление слов. Поэтому для некоторых задач, особенно связанных с семантическим анализом, они могут быть более эффективными.\n",
    "\n",
    "#### Выбор подходящего метода в зависимости от задачи\n",
    "\n",
    "Выбор метода извлечения признаков зависит от конкретной задачи и характеристик текстовых данных. `TF-IDF` подходит хорошо для задач, связанных с извлечением ключевых слов, кластеризацией и классификацией. Однако для более сложных задач, где важны семантические отношения между словами, стоит рассмотреть использование более современных методов, таких как `Word2Vec`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практические советы по использованию TF-IDF\n",
    "#### Подготовка и предобработка текстовых данных\n",
    "<b>Удаление стоп-слов и специальных символов</b>\n",
    "\n",
    "Перед применением `TF-IDF` к текстовым данным, часто полезно провести предварительную обработку данных. Одним из широко распространенных шагов является удаление стоп-слов - слов, которые не несут смысловой нагрузки (например, предлоги, союзы) и специальных символов. Для этого можно использовать библиотеку Natural Language Toolkit `(NLTK)` на Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобработанный текст: машинное обучение это интересная область изучаемая многими\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matveybaksuk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matveybaksuk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Загрузка стоп-слов и пунктуации\n",
    "nltk.download('stopwords') # найдем их локально чтобы посмотреть, что за файл у нас скачался!\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text.lower())  # Привести к нижнему регистру и токенизировать\n",
    "    filtered_words = [word for word in words if word not in stop_words and word not in punctuation]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Пример предобработки текстовых данных\n",
    "text = \"Машинное обучение - это интересная область, изучаемая многими.\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(\"Предобработанный текст:\", preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Приведение к нижнему регистру и лемматизация\n",
    "\n",
    "Кроме удаления стоп-слов, также полезно привести текст к нижнему регистру и выполнить лемматизацию - приведение слов к их базовой форме. Это позволяет уменьшить разнообразие форм слова и улучшить качество извлекаемых признаков. Воспользуемся библиотекой `spaCy` для лемматизации:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобработанный и лемматизированный текст: машинный обучение это интересный область изучаемая многими\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "# Загрузка языковой модели spaCy\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "def preprocess_and_lemmatize(text):\n",
    "    doc = nlp(text.lower())  # Привести к нижнему регистру и лемматизировать\n",
    "    lemmatized_words = [token.lemma_ for token in doc if token.text not in punctuation and token.text not in stop_words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Пример предобработки и лемматизации текста\n",
    "text = \"Машинное обучение - это интересная область, изучаемая многими.\"\n",
    "preprocessed_lemmatized_text = preprocess_and_lemmatize(text)\n",
    "print(\"Предобработанный и лемматизированный текст:\", preprocessed_lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "Использование `TF-IDF` открывает перед нами широкий спектр возможностей для анализа и интерпретации текстовых данных. Совместно с современными методами и инструментами анализа, он помогает в создании информативных моделей и понимании семантических связей в текстах. При использовании правильных методов предобработки и параметров `TF-IDF`, вы сможете добиться высокой точности и эффективности в анализе текстовых данных\n",
    "\n",
    "Также, существуют и другие, более сложные методы для работы с текстами (например, статья про [word2vec](https://habr.com/ru/articles/801807/) описывает одну из самых популярных моделей для этого)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
